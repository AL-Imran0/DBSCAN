# -*- coding: utf-8 -*-
"""DBSCAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15LjFn4a_ZP3thi-rFZh8artvQHdmIOz_
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import cdist
from scipy.linalg import inv
from sklearn.utils import resample
from collections import defaultdict
import warnings

np.random.seed(42)
warnings.filterwarnings("ignore")

def print_header(title):
    line = "=" * len(title)
    print(f"\n{line}\n{title}\n{line}")

def safe_silhouette(X, labels):
    """Silhouette ignoring noise; returns None if <2 clusters after filtering."""
    mask = labels != -1
    if mask.sum() < 2:
        return None
    lbl = labels[mask]
    if len(np.unique(lbl)) < 2:
        return None
    return silhouette_score(X[mask], lbl)

def jaccard_partition(labels1, labels2):
    """
    Jaccard index between two partitions on a common index set.
    Works by pairwise co-membership consistency.
    """
    n = min(len(labels1), len(labels2))
    l1, l2 = labels1[:n], labels2[:n]
    # ignore noise in co-membership (treat -1 as its own label)
    same1 = (l1[:, None] == l1[None, :])
    same2 = (l2[:, None] == l2[None, :])
    # focus on within-cluster pairs only (exclude pairs where either is noise)
    nonnoise_pair = (l1[:, None] != -1) & (l1[None, :] != -1) & (l2[:, None] != -1) & (l2[None, :] != -1)
    inter = np.logical_and(same1, same2) & nonnoise_pair
    union = np.logical_or(same1, same2) & nonnoise_pair
    inter_sum = np.triu(inter, 1).sum()
    union_sum = np.triu(union, 1).sum()
    return inter_sum / union_sum if union_sum > 0 else 0.0

def knee_point(x, y):
    """
    Simple knee detection using maximum distance from the chord method.
    Assumes x is ascending (e.g., k-distance indices), y is monotonic increasing.
    Returns index of knee.
    """
    x = np.asarray(x)
    y = np.asarray(y)
    p1 = np.array([x[0], y[0]])
    p2 = np.array([x[-1], y[-1]])
    line_vec = p2 - p1
    line_len = np.linalg.norm(line_vec)
    if line_len == 0:
        return len(x)//2
    line_unit = line_vec / line_len
    # distances of each point to chord
    diffs = np.stack([x, y], axis=1) - p1
    proj = diffs @ line_unit
    proj_point = np.outer(proj, line_unit) + p1
    dists = np.linalg.norm(np.stack([x, y], axis=1) - proj_point, axis=1)
    return int(np.argmax(dists))

# ==============================================================================
# SECTION 1: SIMULATE EXTENDED DATASET
# ==============================================================================
print_header("1) Simulating Patients & Biomarkers")

n_patients = 800

# Primary biomarkers (3 strata each)
fasting_glucose = np.concatenate([np.random.normal(90, 6, 300),
                                  np.random.normal(115, 10, 250),
                                  np.random.normal(150, 15, 250)])
hba1c = np.concatenate([np.random.normal(5.3, 0.3, 300),
                        np.random.normal(6.1, 0.4, 250),
                        np.random.normal(7.4, 0.7, 250)])
bmi = np.concatenate([np.random.normal(23, 2.0, 300),
                      np.random.normal(28, 3.0, 250),
                      np.random.normal(34, 4.0, 250)])
systolic_bp = np.concatenate([np.random.normal(115, 10, 300),
                              np.random.normal(130, 12, 250),
                              np.random.normal(145, 15, 250)])
chol = np.concatenate([np.random.normal(180, 25, 300),
                       np.random.normal(220, 30, 250),
                       np.random.normal(260, 35, 250)])
tg = np.concatenate([np.random.normal(120, 25, 300),
                     np.random.normal(180, 40, 250),
                     np.random.normal(260, 55, 250)])
hdl = np.concatenate([np.random.normal(55, 10, 300),
                      np.random.normal(45, 9, 250),
                      np.random.normal(35, 8, 250)])
age = np.random.randint(30, 80, n_patients)
insulin = np.concatenate([np.random.normal(8, 2, 300),
                          np.random.normal(15, 4, 250),
                          np.random.normal(25, 6, 250)])  # fasting insulin μU/mL
waist = np.concatenate([np.random.normal(85, 6, 300),
                        np.random.normal(96, 7, 250),
                        np.random.normal(108, 8, 250)])  # cm
family_history = np.random.choice([0,1], size=n_patients, p=[0.6,0.4])
smoking = np.random.choice([0,1], size=n_patients, p=[0.7,0.3])
activity = np.random.choice([0,1,2], size=n_patients, p=[0.35,0.45,0.20])  # 0 sedentary, 1 moderate, 2 active
heart_rate = np.random.randint(58, 102, n_patients)

# Derived indices (clinical formulas)
# LDL (Friedewald): LDL ≈ TC - HDL - TG/5  (mg/dL)
ldl = chol - hdl - tg/5.0

# HOMA-IR = (Glucose (mg/dL) * Insulin (μU/mL)) / 405
homa_ir = (fasting_glucose * insulin) / 405.0

# TG/HDL ratio & Atherogenic Index of Plasma: AIP = log10(TG/HDL)
tg_hdl = tg / np.clip(hdl, 1e-6, None)
aip = np.log10(tg_hdl)

# Metabolic syndrome component count (NCEP ATP III style; simplified thresholds)
mets_count = (
    (waist > 102).astype(int) +                         # waist
    (tg >= 150).astype(int) +                           # triglycerides
    (hdl < 40).astype(int) +                            # HDL (men threshold; simplified)
    (systolic_bp >= 130).astype(int) +                  # BP
    (fasting_glucose >= 100).astype(int)                # fasting glucose
)

columns = [
    'Fasting_Glucose','HbA1c','BMI','Systolic_BP','Cholesterol',
    'Triglycerides','HDL','LDL','HOMA_IR','AIP','TG_to_HDL',
    'Age','Insulin','Waist','Family_History','Smoking','Activity','Heart_Rate','MetS_Count'
]
X = np.column_stack([
    fasting_glucose, hba1c, bmi, systolic_bp, chol,
    tg, hdl, ldl, homa_ir, aip, tg_hdl,
    age, insulin, waist, family_history, smoking, activity, heart_rate, mets_count
])
data = pd.DataFrame(X, columns=columns)

print(data.head())

# ==============================================================================
# SECTION 2: PREPROCESSING & OPTIONS
# ==============================================================================
print_header("2) Preprocessing & Options")

scaler_choice = "robust"  # "standard" or "robust"
if scaler_choice == "standard":
    scaler = StandardScaler()
else:
    scaler = RobustScaler(quantile_range=(10, 90))

X_scaled = scaler.fit_transform(data)

# Optional feature weighting (increase cardiometabolic emphasis)
feature_weights = {
    'Fasting_Glucose':1.2, 'HbA1c':1.3, 'BMI':1.1, 'Systolic_BP':1.0,
    'Cholesterol':0.9, 'Triglycerides':1.0, 'HDL':1.0, 'LDL':1.0,
    'HOMA_IR':1.2, 'AIP':1.1, 'TG_to_HDL':1.0, 'Age':0.8, 'Insulin':1.0,
    'Waist':1.1, 'Family_History':0.7, 'Smoking':0.8, 'Activity':0.7,
    'Heart_Rate':0.7, 'MetS_Count':1.2
}
w = np.array([feature_weights[c] for c in columns])
Xw = X_scaled * w  # weighted feature space

# ==============================================================================
# SECTION 3: ADAPTIVE EPS SUGGESTION VIA K-DISTANCE KNEE
# ==============================================================================
print_header("3) Adaptive ε Suggestion via k-distance Knee")

min_samples_default = 8  # rule-of-thumb: ≥ dimensionality*2
nbrs = NearestNeighbors(n_neighbors=min_samples_default, metric='euclidean').fit(Xw)
dists, _ = nbrs.kneighbors(Xw)
k_dists = np.sort(dists[:, -1])  # distance to kth neighbor
idx = np.arange(1, len(k_dists)+1)
knee_idx = knee_point(idx, k_dists)
eps_suggest = float(k_dists[knee_idx])  # data-driven suggestion

print(f"Suggested eps (knee of k-distance plot @ k={min_samples_default}): {eps_suggest:.3f}")

plt.figure(figsize=(9,5))
plt.plot(idx, k_dists, lw=1.5)
plt.scatter([idx[knee_idx]], [k_dists[knee_idx]], s=60, marker='o')
plt.title("k-Distance Plot (k = min_samples) & Knee")
plt.xlabel("Points sorted by distance")
plt.ylabel(f"k-distance (k={min_samples_default})")
plt.grid(True)
plt.show()

# ==============================================================================
# SECTION 4: DISTANCE METRIC OPTIONS
# ==============================================================================
print_header("4) Distance Metrics: Euclidean & Mahalanobis")

# Mahalanobis requires inverse covariance
cov = np.cov(Xw.T)
# Regularize in case of near-singularity
cov += np.eye(cov.shape[0]) * 1e-6
VI = inv(cov)

# ==============================================================================
# SECTION 5: GRID SEARCH OVER EPS × MIN_SAMPLES × METRIC
# ==============================================================================
print_header("5) Grid Search & Model Selection")

eps_grid = np.linspace(max(0.6, eps_suggest*0.7), eps_suggest*1.6, 7)
min_samples_grid = [6, 8, 10, 12]
metrics = [
    ("euclidean", None),
    ("mahalanobis", {"VI": VI})
]

results = []
best = {"score": -np.inf, "params": None, "labels": None}

for metric_name, metric_params in metrics:
    for ms in min_samples_grid:
        for eps in eps_grid:
            db = DBSCAN(eps=float(eps), min_samples=int(ms), metric=metric_name, metric_params=metric_params)
            labels = db.fit_predict(Xw)
            score = safe_silhouette(Xw, labels)
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = int((labels == -1).sum())
            results.append((metric_name, ms, float(eps), score if score is not None else -1, n_clusters, n_noise))
            if score is not None and score > best["score"]:
                best.update(score=score, params=(metric_name, ms, float(eps)), labels=labels)

results_df = pd.DataFrame(results, columns=["metric","min_samples","eps","silhouette","clusters","noise"])
print(results_df.sort_values("silhouette", ascending=False).head(10))

if best["params"] is None:
    # fallback: pick a model with max clusters if silhouettes invalid
    pick = results_df.sort_values(["clusters","noise"], ascending=[False, True]).iloc[0]
    chosen_metric, chosen_ms, chosen_eps = pick["metric"], int(pick["min_samples"]), float(pick["eps"])
    final_labels = DBSCAN(eps=chosen_eps, min_samples=chosen_ms,
                          metric=chosen_metric, metric_params=(None if chosen_metric=='euclidean' else {"VI":VI})
                         ).fit_predict(Xw)
    final_score = safe_silhouette(Xw, final_labels)
else:
    chosen_metric, chosen_ms, chosen_eps = best["params"]
    final_labels = best["labels"]
    final_score = best["score"]

print(f"\nChosen: metric={chosen_metric}, min_samples={chosen_ms}, eps={chosen_eps:.3f}, silhouette={None if final_score is None else round(final_score,3)}")
data["Cluster"] = final_labels

# Heatmap of silhouette across grid (per metric)
for metric_name in {m for m,_ in metrics}:
    sub = results_df[results_df.metric==metric_name]
    eps_vals = sorted(sub.eps.unique())
    ms_vals = sorted(sub.min_samples.unique())
    heat = np.full((len(ms_vals), len(eps_vals)), np.nan)
    for i, ms in enumerate(ms_vals):
        for j, eps in enumerate(eps_vals):
            s = sub[(sub.min_samples==ms)&(sub.eps==eps)]["silhouette"].values
            heat[i,j] = s[0] if len(s) else np.nan
    plt.figure(figsize=(10,5))
    plt.imshow(heat, aspect="auto", origin="lower", extent=[min(eps_vals), max(eps_vals), min(ms_vals), max(ms_vals)])
    plt.colorbar(label="Silhouette (noise ignored)")
    plt.xlabel("eps")
    plt.ylabel("min_samples")
    plt.title(f"Silhouette Heatmap — metric={metric_name}")
    plt.show()

# ==============================================================================
# SECTION 6: CLUSTER PROFILING & RISK LABELING (MATHEMATICAL)
# ==============================================================================
print_header("6) Cluster Profiling & Risk Labels")

def cluster_risk_label(df):
    # Rule-based on population means of key biomarkers (transparent math)
    g = df["Fasting_Glucose"].mean()
    a1c = df["HbA1c"].mean()
    mets = df["MetS_Count"].mean()
    homa = df["HOMA_IR"].mean()
    if (g < 100 and a1c < 5.7 and mets < 1.5 and homa < 2.0):
        return "Low Risk"
    elif (g < 126 and a1c < 6.5 and mets < 3 and homa < 3.5):
        return "Moderate Risk"
    elif (g < 160 and a1c < 7.5) or (mets >= 3):
        return "High Risk"
    else:
        return "Extreme Risk"

cluster_stats = []
for cid in sorted([c for c in np.unique(final_labels) if c != -1]):
    dfc = data[data.Cluster==cid]
    summary = {
        "Cluster": cid,
        "Patients": len(dfc),
        "Risk_Label": cluster_risk_label(dfc),
        "FGlu": dfc["Fasting_Glucose"].mean(),
        "HbA1c": dfc["HbA1c"].mean(),
        "BMI": dfc["BMI"].mean(),
        "SBP": dfc["Systolic_BP"].mean(),
        "TG": dfc["Triglycerides"].mean(),
        "HDL": dfc["HDL"].mean(),
        "LDL": dfc["LDL"].mean(),
        "HOMA_IR": dfc["HOMA_IR"].mean(),
        "AIP": dfc["AIP"].mean(),
        "MetS": dfc["MetS_Count"].mean()
    }
    cluster_stats.append(summary)

cluster_df = pd.DataFrame(cluster_stats).sort_values("Cluster")
print(cluster_df)

# Percentile profile (useful clinically)
key_feats = ["Fasting_Glucose","HbA1c","BMI","Systolic_BP","Triglycerides","HDL","LDL","HOMA_IR","AIP","MetS_Count"]
percentile_profiles = []
for cid in cluster_df.Cluster:
    dfc = data[data.Cluster==cid]
    prof = {"Cluster": cid}
    for feat in key_feats:
        prof[f"{feat}_p50"] = np.median(dfc[feat])
        prof[f"{feat}_p90"] = np.percentile(dfc[feat], 90)
        prof[f"{feat}_p10"] = np.percentile(dfc[feat], 10)
    percentile_profiles.append(prof)
percent_df = pd.DataFrame(percentile_profiles)
print("\nPercentile Profiles (p10/p50/p90):")
print(percent_df.iloc[:, :1+3*5])  # print subset to keep console readable

# ==============================================================================
# SECTION 7: VISUALIZATIONS
# ==============================================================================
print_header("7) Visualizations")

# PCA projection (2D)
pca = PCA(n_components=2)
Xp = pca.fit_transform(Xw)
plt.figure(figsize=(10,7))
scatter = plt.scatter(Xp[:,0], Xp[:,1], c=final_labels, cmap='tab10', s=18, alpha=0.8)
plt.title(f"PCA Projection (metric={chosen_metric}, eps={chosen_eps:.2f}, ms={chosen_ms})")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.colorbar(scatter, label="Cluster")
plt.grid(True)
plt.show()

# Radar charts per cluster (normalized to dataset median/IQR)
def radar_plot(df_clusters, features, title):
    # robust scaling by median/IQR for interpretability
    med = data[features].median()
    iqr = data[features].quantile(0.75) - data[features].quantile(0.25)
    iqr = iqr.replace(0, 1.0)
    angles = np.linspace(0, 2*np.pi, len(features), endpoint=False)
    angles = np.concatenate([angles, [angles[0]]])

    plt.figure(figsize=(8,8))
    ax = plt.subplot(111, polar=True)
    for _, row in df_clusters.iterrows():
        cid = int(row["Cluster"])
        vals = []
        for f in features:
            v = data.loc[data.Cluster==cid, f].mean()
            vals.append(((v - med[f]) / iqr[f]))  # robust z
        vals.append(vals[0])
        ax.plot(angles, vals, linewidth=2, label=f"Cluster {cid} ({row['Risk_Label']})")
        ax.fill(angles, vals, alpha=0.12)
    ax.set_thetagrids(angles[:-1]*180/np.pi, features)
    ax.set_title(title)
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    plt.show()

radar_features = ["Fasting_Glucose","HbA1c","BMI","Systolic_BP","Triglycerides","HDL","HOMA_IR","MetS_Count"]
radar_plot(cluster_df, radar_features, "Cluster Radar Profiles (median/IQR scaled)")

# ==============================================================================
# SECTION 8: STABILITY VIA BOOTSTRAP JACCARD
# ==============================================================================
print_header("8) Cluster Stability (Bootstrap Jaccard)")

B = 3
jac = []
for b in range(B):
    boot = resample(Xw, n_samples=int(0.8*len(Xw)), replace=True, random_state=17+b)
    if chosen_metric == "euclidean":
        labels_b = DBSCAN(eps=chosen_eps, min_samples=chosen_ms, metric='euclidean').fit_predict(boot)
    else:
        labels_b = DBSCAN(eps=chosen_eps, min_samples=chosen_ms, metric='mahalanobis', metric_params={"VI":VI}).fit_predict(boot)
    # Compare partitions across bootstrap runs pairwise
    if b > 0:
        jac.append(jaccard_partition(prev_labels, labels_b))
    prev_labels = labels_b

if jac:
    print(f"Mean Jaccard across {B-1} comparisons: {np.mean(jac):.3f}")
else:
    print("Insufficient bootstrap comparisons.")

# ==============================================================================
# SECTION 9: PATIENT-LEVEL EXPLANATION
# ==============================================================================
print_header("9) Patient-level Explanation (first 5 patients)")

# Compute cluster centroids in weighted space
centroids = {}
for cid in sorted([c for c in np.unique(final_labels) if c != -1]):
    idx = np.where(final_labels==cid)[0]
    centroids[cid] = Xw[idx].mean(axis=0)
centroid_mat = np.stack([centroids[c] for c in sorted(centroids.keys())])
centroid_ids = np.array(sorted(centroids.keys()))

def explain_patient(i):
    x = Xw[i:i+1]
    d2c = cdist(x, centroid_mat, metric='euclidean').ravel()
    nearest = centroid_ids[np.argmin(d2c)]
    # z-score style explanation (robust)
    med = np.median(X, axis=0)
    mad = np.median(np.abs(X - med), axis=0)
    mad[mad==0] = 1.0
    robust_z = (X[i] - med) / mad
    return nearest, d2c.min(), robust_z

for i in range(5):
    cid, d, rz = explain_patient(i)
    print(f"Patient {i}: assigned Cluster={data.Cluster.iloc[i]}, nearest centroid={cid}, dist={d:.3f}")
    explain_pairs = list(zip(columns, rz))
    explain_pairs.sort(key=lambda kv: -abs(kv[1]))
    print("  Top contributing biomarkers (robust z):")
    for name, z in explain_pairs[:5]:
        print(f"   - {name}: z≈{z:.2f}")

# ==============================================================================
# SECTION 10: NOISE / OUTLIER ANALYSIS
# ==============================================================================
print_header("10) Noise / Outlier Analysis")

noise_idx = np.where(final_labels == -1)[0]
print(f"Noise patients: {len(noise_idx)} ({len(noise_idx)/len(final_labels):.1%})")
if len(noise_idx) > 0:
    noisy = data.iloc[noise_idx]
    print("Noise summary (means of key biomarkers):")
    print(noisy[["Fasting_Glucose","HbA1c","BMI","Systolic_BP","Triglycerides","HDL","HOMA_IR","MetS_Count"]].mean())

    # Show where noise sits in PCA space
    plt.figure(figsize=(9,6))
    plt.scatter(Xp[:,0], Xp[:,1], c=(final_labels!=-1), cmap='coolwarm', s=18)
    plt.title("Noise vs Clustered Patients in PCA Space (blue=noise)")
    plt.xlabel("PC1"); plt.ylabel("PC2"); plt.grid(True)
    plt.show()

# ==============================================================================
# SECTION 11: FINAL CONCISE TAKEAWAY
# ==============================================================================
print_header("11) Final Takeaway")
n_clusters_final = len(set(final_labels)) - (1 if -1 in final_labels else 0)
print(f"Final clusters: {n_clusters_final} | Metric: {chosen_metric} | eps: {chosen_eps:.3f} | min_samples: {chosen_ms} | Silhouette: {None if final_score is None else round(final_score,3)}")
print("Risk by cluster:")
for _, r in cluster_df.iterrows():
    print(f" - Cluster {int(r['Cluster'])}: {r['Risk_Label']} (n={int(r['Patients'])}) — "
          f"Avg FGlu {r['FGlu']:.0f}, HbA1c {r['HbA1c']:.2f}, MetS {r['MetS']:.2f}")